{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "warnings.warn = warn\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from tensorforce.agents import Agent\n",
    "from tensorforce.execution import Runner\n",
    "from tensorforce.contrib.openai_gym import OpenAIGym\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.abspath('')))\n",
    "\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "from tensortrade.exchanges.live import CCXTExchange\n",
    "from tensortrade.actions import DiscreteActionStrategy\n",
    "from tensortrade.rewards import SimpleProfitStrategy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Exchange\n",
    "coinbase = ccxt.coinbasepro()\n",
    "exchange = CCXTExchange(exchange=coinbase, base_instrument='USD')\n",
    "\n",
    "# defining the action strategy\n",
    "action_strategy = DiscreteActionStrategy()\n",
    "\n",
    "# Defining the reward strategy\n",
    "reward_strategy = SimpleProfitStrategy()\n",
    "\n",
    "# configuring the trading environment\n",
    "env = TradingEnvironment(exchange=exchange,\n",
    "                         action_strategy=action_strategy,\n",
    "                         reward_strategy=reward_strategy,\n",
    "                         feature_pipeline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the feature pipeline and also seting it up with the coinbase exchange.\n",
    "\n",
    "from tensortrade.features import FeaturePipeline\n",
    "from tensortrade.features.scalers import MinMaxNormalizer\n",
    "from tensortrade.features.stationarity import FractionalDifference\n",
    "from tensortrade.features.indicators import SimpleMovingAverage\n",
    "price_columns = [\"open\", \"high\", \"low\", \"close\"]\n",
    "normalize_price = MinMaxNormalizer(price_columns)\n",
    "moving_averages = SimpleMovingAverage(price_columns)\n",
    "difference_all = FractionalDifference(difference_order=0.6)\n",
    "feature_pipeline = FeaturePipeline(steps=[normalize_price,\n",
    "                                          moving_averages,\n",
    "                                          difference_all])\n",
    "# setting the exchange \n",
    "exchange.feature_pipeline = feature_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the agent\n",
    "agent_config = {\n",
    "    \"type\": \"dqn_agent\",\n",
    "\n",
    "    \"update_mode\": {\n",
    "        \"unit\": \"timesteps\",\n",
    "        \"batch_size\": 64,\n",
    "        \"frequency\": 4\n",
    "    },\n",
    "    \n",
    "    \"memory\": {\n",
    "        \"type\": \"replay\",\n",
    "        \"capacity\": 10000,\n",
    "        \"include_next_states\": True\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"clipped_step\",\n",
    "        \"clipping_value\": 0.1,\n",
    "        \"optimizer\": {\n",
    "            \"type\": \"adam\",\n",
    "            \"learning_rate\": 1e-3\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"discount\": 0.999,\n",
    "    \"entropy_regularization\": None,\n",
    "    \"double_q_model\": True,\n",
    "\n",
    "    \"target_sync_frequency\": 1000,\n",
    "    \"target_update_weight\": 1.0,\n",
    "\n",
    "    \"actions_exploration\": {\n",
    "        \"type\": \"epsilon_anneal\",\n",
    "        \"initial_epsilon\": 0.5,\n",
    "        \"final_epsilon\": 0.,\n",
    "        \"timesteps\": 1000000000\n",
    "    },\n",
    "\n",
    "    \"saver\": {\n",
    "        \"directory\": None,\n",
    "        \"seconds\": 600\n",
    "    },\n",
    "    \"summarizer\": {\n",
    "        \"directory\": None,\n",
    "        \"labels\": [\"graph\", \"total-loss\"]\n",
    "    },\n",
    "    \"execution\": {\n",
    "        \"type\": \"single\",\n",
    "        \"session_config\": None,\n",
    "        \"distributed_spec\": None\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network specifications\n",
    "network_spec = [\n",
    "    dict(type='dense', size=64),\n",
    "    dict(type='dense', size=32)\n",
    "]\n",
    "\n",
    "agent = Agent.from_spec(\n",
    "        spec=agent_config,\n",
    "        kwargs=dict(\n",
    "            states=env.states,\n",
    "            actions=env.actions,\n",
    "            network=network_spec,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create the runner\n",
    "runner = Runner(agent=agent, environment=env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Callback function printing episode statistics\n",
    "def episode_finished(r):\n",
    "    print(\"Finished episode {ep} after {ts} timesteps (reward: {reward})\".format(ep=r.episode, ts=r.episode_timestep,\n",
    "                                                                                 reward=r.episode_rewards[-1]))\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Start learning\n",
    "runner.run(episodes=300, max_episode_timesteps=10000, episode_finished=episode_finished)\n",
    "runner.close()\n",
    "\n",
    "# Print statistics\n",
    "print(\"Learning finished. Total episodes: {ep}. Average reward of last 100 episodes: {ar}.\".format(\n",
    "    ep=runner.episode,\n",
    "    ar=np.mean(runner.episode_rewards))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
